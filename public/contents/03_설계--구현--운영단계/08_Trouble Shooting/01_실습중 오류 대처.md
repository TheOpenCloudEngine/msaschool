## 실습 중 공통적 오류에 대한 대처
<details>
<summary>오타 (Typo) 인지 먼저 확인</summary>

  - 명령어, 파일명, 유알엘, 소스코드 등이 정확한지
  - 직접 입력하지 말고 복사해서 명령을 다시 입력해 볼 것
</details>

<details>
<summary>기본적 pod 상태 확인</summary>

  - 상태점검:   kubectl get po  
  - pod 들 중 다음상태인 것을 확인
    - Pending:  클러스터 워커 노드 개수 부족할때 pod 를 디플로이할 공간이 없다.
      - (조치방법)
      - 클러스터 개수를 늘려줌
        - (GCP)
        - 상단의 검색 > kubernetes 클러스터 선택
        - 해당 클러스터 선택 > 수정
        - 노드수 --> 늘여줌
        - 저장.
      - or
      - 쓰지않는 deployment 들을 delete (kubectl delete deploy ...)
    - ImagePullBackOff:   타이핑 실수!!  --- 이미지 이름 잘못 입력
      - GCR 에 정확히 업로드 되어있는지 확인
        - 상단의 검색 > gcr 
        - gcr 레지스트리 내에 해당 이미지가 올려져 있는지 확인
</details>

<details>
<summary>서버 애플리케이션 접속이 안될때의 트러블 슈팅 방법</summary>

  - 첫번째 해당 pod 의 log 를 확인한다.
    - kubectl logs [pod 이름] -f
    - Pod 상태가 ImagePullBackOff 
      - --> 이미지를 못 찾아내린 경우. 이미지가 GCR에 올라가지 않았거나, 혹은 이미지명이 잘못된 경우.
  - 두번째 해당 서비스가 localhost 로 접근이 되는가를 해당 컨테이너 내부에서 확인
    - kubectl exec -it [복사된 pod 이름] -- /bin/bash
    - curl localhost
    - –위와 같이 확인했을때 서비스가 연결된다면, 다음단계 확인
  - 세번째 해당 서비스의 도커이미지에서 Port 설정이 제대로 되었는가?
    - –e.g.
    - –Nginx 서비스는 80으로 노출되었는데, Dockerfile 에서
    - –EXPOSE 8080
    - –으로 설정되었다면, 해당 서비스는 도커 컨테이너 밖으로 포트를 노출하지 못함
    - –EXPOSE 80  으로 수정해야 함
  - 네번째 해당 쿠버네티스 Service yaml 설정에서 port 넘버가 잘못된 경우
    - 도커이미지의 포트넘버를 쿠버네티스의 Service yaml 이 자동으로 가져오지 못하므로
    - Service yaml 의 spec 부분을 아래와 같이 점검:
    - spec:
      - port: 80    # 이 부분을 확인 도커파일에서 EXPOSE 한 포트와 동일한지 확인
      - protocol: TCP
      - targetPort: 80
</details>

<details>
<summary>kubectl 실행시 Connection fail 오류 (dial up... 어쩌구)</summary>

  - 원인: kubectl 클라이언트가 kubernetes cluster 접속이 안된 문제.
  - 해결책:  kubectl 을 위한 Credential 파일 생성을 해주어야 함

- ImagePullBackOff
  - 이미지 명이 kubectl create deploy --image=... 부분이나 Deployment.yaml 등에 (--image:.... 부분) 잘못 기입된 경우
  - 원인
    - 이미지를 못찾는 경우
      - 명칭이 잘못된 경우
        - 이름을 정확히 확인
      - 클라우드에 정확히 push 되지 않은 경우
        - GCR 페이지 들어가서 정확히 확인
    - gcr.io/<project id>/class-course 의 <project id> 를 제대로 못쓴 경우
</details>

<details>
<summary>CrashLoopBackOff</summary>

  - 서비스가 올라오지 않은 경우
  - 디펜던시 서비스 중 하나가 올라오지 않은 경우
    - 예) Kafka 서비스가 올라오지 않은 경우
      - helm install --name education-kafka 로 "name"을 정확히 넣어주지 않은 경우
      - 조치: 실습 > "(Kafka 설치)" 부분을 정확히 따라서 입력
      - 부가적으로 
        - helm del my-kafka --purge 입력하여 불필요한 리소스 제거
  - 조치: 로그 확인
    - kubectl logs <pod name> -f
    - 그래도 안되면, 컨테이너에 들어가서 확인
      - kubectl exec -it <pod name> -- /bin/bash
</details>

<details>
<summary>Pending Pod</summary>

  - Pod 의 status 가 pending 에서 변하지 않음
  - 주로, worker node 갯수가 부족한 경우
  - 해결
    - node 를 추가.. kubernetes cluster 설정에서 "수정" > node 개수 추가 > "저장" > 몇 분 기다려야 함... 
    - 혹은 쓰지 않는 deployment 들을 삭제
      - kubectl delete deploy {안쓰는 디플로이}
</details>

<details>
<summary>Evicted Pod</summary>

  - 원인
    - 이미 스케쥴링 되었던 pod 가 들어간 노드에 메모리나 CPU 가 부족해지면 기존에 잘 돌던 pod 가 쫓겨나는 경우가 생김
      - 엘리베이터에 타고 올라가는 중에 만원이 되어 중간 층에서 쫓겨나는 상황
    - pod 자체의 사이즈가 커서 어느 node 도 받아들일 수 없는 경우 --> 노드 스케일 업 필요
  - 해결방법:
    - 노드를 스케일 업 한 후 (e.g. v-cpu 1 개 짜리 --> v-cpu 8 짜리로 업그래이드), 
    - evicted pod 를 제거해주면 다시 스케쥴링됨.
      - Evicted 된 pod 의 제거:
        - kubectl get pods --all-namespaces -ojson | jq -r '.items[] | select(.status.reason!=null) | select(.status.reason | contains("Evicted")) | .metadata.name + " " + .metadata.namespace' | xargs -n2 -l bash -c 'kubectl delete pods $0 --namespace=$1'
    - 그러면, 알아서 다시 pod 가 스케쥴링이 되어 해결됨
    - [!] 사이즈가 커서 들어갈 노드가 없는 경우라서 작은 사이즈의 노드를 아무리 스케일 아웃해도 해결이 안됨
    - 이걸 매번 하는게 귀찮으면, 쿠버의 설정을 해주면 된다고 함: https://github.com/kubernetes/kubernetes/issues/55051
    - https://www.facebook.com/groups/k8skr/permalink/2339818716299788/
</details>

<details>
<summary>kubectl TLS skip 옵션</summary>

  - --insecure-skip-tls-verify=true
</details>

<details>
<summary>리소스 한도에 의한 문제</summary>

  - OOM
  - https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#node-oom-behavior

- Helm 명령이 없다고 나올때
  - 아래 입력:
    - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash 
  - helm 이 아예 설치되지 않은 경우, helm 설치:
    - https://workflowy.com/s/helm/Hp7x4R4woJOpzMlA

- helm install kafka 할때 오류가 나는 경우  : default namespace 에 설치 못함.... 등등 오류
  - 레포지토리 등록이 안된경우:
    - helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator 
    - 로 추가해줌
  - helm 이 제대로 설치 안된 경우
  - 특히, helm 의 tiller 에게 권한이 부여되지 않은 경우
  - 조치:  맨 하단의 helm 설치를 제대로 다시 해줌.
  - Forbidden 오류 확인 시,
    - kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
</details>

<details>
<summary>helm 으로 설치한 서비스들이 잘못 꼬인 경우 다시 설치</summary>

  - helm del <서비스명> --purge
    - e.g. helm del nginx-ingress --purge
    - helm install .... 
</details>

<details>
<summary>git 자격증명 오류</summary>

  - Windows 서비스 검색(돋보기)에서 자격 증명 관리자 실행
  - Windows 자격증명에서 https://github~~ 찾아서 삭제
</details>

<details>
<summary>Pod Advanced Operation</summary>

  - pod 강제 삭제 : kubectl delete pod (foo) --grace-period=0 --force
  - kubectl get pods --field-selector status.phase=Pending
</details>

<details>
<summary>Windows  포트 점유 프로세스 id 스캔 및 프로세스 종료하기</summary>

  - netstat -ano | findstr "PID :808"
    - 명령어 수행결과, 8080을 사용하는 pid가 18264라고 할 때, 이를 kill 한다.
  - taskkill /pid 18264 /f
</details>

<details>
<summary>Kafka Error Trouble Shooting</summary>

  - Kafka Server가 정상적으로 기동되지 않는 경우, 아래 명령행 실행 후 재시작
    "rm -rf /tmp/kafka*
    rm -rf  /tmp/zookeeper*
    "
  - ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager) 가 계속적으로 발생하는 경우, 아래 명령으로 토픽 삭제
    "zookeeper-shell.sh localhost:2181
    ls /brokers/topics 
    토픽이름 토픽2 토픽3
    deleteall /brokers/topics/토픽이름"
</details>

<details>
<summary>ubuntu root로 로그인하여, 사용자계정 비밀번호 초기화</summary>

  - ubuntu1804 config --default-user root
  - passwd USER-ACCOUNT
  - ubuntu1804 config --default-user USER-ACCOUNT
</details>

<details>
<summary>Alpine Linux Pod(컨테이너) 접속</summary>

  - kubectl exec -it pod/~~ -- /bin/sh
  - root $> apk add curl
</details>
